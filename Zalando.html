<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>FashionBrain Showcases</title>

    <meta name="description" content="FashionBrain Showscases">
    <meta name="author" content="Marc Bonne & Iana Gein">

    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/style.css" rel="stylesheet">

  </head>

<!-- Navigation bar-->

<nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="navbar">

<div class="menuLogo"></div>
  <a class="navbar-brand" href="index.html">Showcases</a>

      <ul class="navbar-nav ml-auto">
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="Beuth.html">Relation extraction</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="Fashwell.html">Shop the Look</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="Zalando.html">End-to-End search</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="MonetDB.html">Integrated architecture</a>
        </li>
      </ul>
</nav>
<!-- Navigation bar -->

<!-- Main body -->
  <body>

    <div class="container-fluid text-center">
      <h1> End-to-end product search</h1>
      <p>
      This is a demo of the end-to-end product search engine developed in FashionBrain work package 6.
      </p>
      <p>
      Given a full text query such das "rotes Kleid" (eng. "red dress"), our approach retrieves matching product images. You can try different queries in the demo and visually inspect retrieved images. This demo retrieves images from the test split of the Feidegger dataset (10% of data, 879 images) and is trained specifically for queries in German language,
      but the model is scalable to multilingual queries using the crosslingual embeddings supplied in Flair and multilingual datasets.
      </p>
      <p>
      The underlying model employs a "two-tower" architecture in which each tower embeds one modality (i.e. full text queries and product images) into a shared embedding space.
      The embedding network for full text queries in the demo model is a bidirectional Gated Recurrent Unit (GRU) on top of pre-trained character-based LSTM (Long Short-Term Memory) embeddings for words. The embedding network for images is a shallow (3-layer) Convolutional Neural Net (CNN).
      </p>
      <p>
      The entire architecture is trained "end-to-end" to minimize a similarity cost function with supervised learning over paired image-text datasets.
      For the demo model, we used a training split of Feidegger (80% of all data, 7034 images) which is open source and thus publicly available, allowing reproduction of our results.  The model is implemented and trained in <a href="https://github.com/zalandoresearch/flair">Flair</a> framework.
      </p>
    </div>


    <!-- iframe -->

<!-- iframe -->




	  <!-- Footer -->
    <div class="copyright py-4 text-center text-secondary">
      <div class="container">
        <p>
        Flair from Zalando Research is available at <a href="https://github.com/zalandoresearch/flair">this</a> link.
        </p>
        <small>Copyright &copy; <a href="https://fashionbrain-project.eu"> FashionBrain</a> 2020 </small>
      </div>
    </div>
<!-- Footer -->

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/scripts.js"></script>
  </body>
</html>
